{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ar4857/Data_Analysis-/blob/main/GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UvVI807kDNye",
        "outputId": "3a1ea836-4cd5-47e7-dce6-8529cb7ca8b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow numpy matplotlib opencv-python tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "zUgh8YBA2CbJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff8d9bdb-8d19-4e0f-c854-c96d731c5fc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model\n"
      ],
      "metadata": {
        "id": "0CHNyEvY2jjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths (Modify these)\n",
        "PAIRED_DIR = \"/content/drive/MyDrive/GAN/paired_dataset_art\"  # Path to paired images\n",
        "\n",
        "IMG_SIZE = 256\n",
        "\n",
        "def load_images_from_folder(folder):\n",
        "    images = []\n",
        "    files = sorted(os.listdir(folder))  # Sorting ensures correct pair matching\n",
        "    for file in tqdm(files[:500]):  # Load first 500 images (adjust as needed)\n",
        "        img = cv2.imread(os.path.join(folder, file))\n",
        "        if img is not None:\n",
        "            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "            img = img / 127.5 - 1  # Normalize to [-1, 1]\n",
        "            images.append(img)\n",
        "    return np.array(images)\n",
        "\n",
        "# Load only paired images (damaged & restored)\n",
        "damaged_images = load_images_from_folder(os.path.join(PAIRED_DIR, \"damaged\"))\n",
        "restored_images = load_images_from_folder(os.path.join(PAIRED_DIR, \"undamaged\"))\n",
        "\n",
        "print(f\"Loaded {len(damaged_images)} damaged images and {len(restored_images)} restored images.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NKFyJC22rId",
        "outputId": "5ba6f0f7-7b05-4cd7-f5d3-c04255cbe403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 116/116 [00:03<00:00, 37.44it/s]\n",
            "100%|██████████| 117/117 [00:04<00:00, 23.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 116 damaged images and 117 restored images.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_generator():\n",
        "    inputs = Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "    def conv_block(x, filters, apply_batchnorm=True):\n",
        "        x = Conv2D(filters, kernel_size=4, strides=2, padding='same', use_bias=False)(x)\n",
        "        if apply_batchnorm:\n",
        "            x = BatchNormalization()(x)\n",
        "        x = LeakyReLU()(x)\n",
        "        return x\n",
        "\n",
        "    def deconv_block(x, skip_input, filters, dropout=False):\n",
        "        x = Conv2DTranspose(filters, kernel_size=4, strides=2, padding='same', use_bias=False)(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        if dropout:\n",
        "            x = Dropout(0.5)(x)\n",
        "        x = Concatenate()([x, skip_input])\n",
        "        x = ReLU()(x)\n",
        "        return x\n",
        "\n",
        "    down1 = conv_block(inputs, 64, apply_batchnorm=False)\n",
        "    down2 = conv_block(down1, 128)\n",
        "    down3 = conv_block(down2, 256)\n",
        "    down4 = conv_block(down3, 512)\n",
        "    down5 = conv_block(down4, 512)\n",
        "\n",
        "    bottleneck = conv_block(down5, 512, apply_batchnorm=False)\n",
        "\n",
        "    up1 = deconv_block(bottleneck, down5, 512, dropout=True)\n",
        "    up2 = deconv_block(up1, down4, 512, dropout=True)\n",
        "    up3 = deconv_block(up2, down3, 256)\n",
        "    up4 = deconv_block(up3, down2, 128)\n",
        "    up5 = deconv_block(up4, down1, 64)\n",
        "\n",
        "    outputs = Conv2DTranspose(3, 4, strides=2, padding='same', activation='tanh')(up5)\n",
        "\n",
        "    return Model(inputs, outputs)\n",
        "\n",
        "generator = build_generator()\n",
        "generator.summary()"
      ],
      "metadata": {
        "id": "Fc8LS--1dFZv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_discriminator():\n",
        "    inputs = Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "    target = Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "    x = Concatenate()([inputs, target])\n",
        "\n",
        "    down1 = Conv2D(64, 4, strides=2, padding='same')(x)\n",
        "    down1 = LeakyReLU()(down1)\n",
        "\n",
        "    down2 = Conv2D(128, 4, strides=2, padding='same')(down1)\n",
        "    down2 = BatchNormalization()(down2)\n",
        "    down2 = LeakyReLU()(down2)\n",
        "\n",
        "    down3 = Conv2D(256, 4, strides=2, padding='same')(down2)\n",
        "    down3 = BatchNormalization()(down3)\n",
        "    down3 = LeakyReLU()(down3)\n",
        "\n",
        "    down4 = Conv2D(512, 4, strides=2, padding='same')(down3)\n",
        "    down4 = BatchNormalization()(down4)\n",
        "    down4 = LeakyReLU()(down4)\n",
        "\n",
        "    patch_output = Conv2D(1, 4, strides=1, padding='same', activation='sigmoid')(down4)\n",
        "\n",
        "    return Model([inputs, target], patch_output)\n",
        "\n",
        "discriminator = build_discriminator()\n",
        "discriminator.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "f7lDjB0ddFdM",
        "outputId": "75d7698e-643f-485d-dffd-3fe5b32e5057"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Input' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-b2ca1840ec6f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mdiscriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-b2ca1840ec6f>\u001b[0m in \u001b[0;36mbuild_discriminator\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Input' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Pix2PixGAN(tf.keras.Model):\n",
        "    def _init_(self, generator, discriminator, lambda_reconstruction=100):\n",
        "        super(Pix2PixGAN, self)._init_()\n",
        "        self.generator = generator\n",
        "        self.discriminator = discriminator\n",
        "        self.lambda_reconstruction = lambda_reconstruction\n",
        "\n",
        "    def compile(self, gen_optimizer, disc_optimizer, loss_fn):\n",
        "        super(Pix2PixGAN, self).compile()\n",
        "        self.gen_optimizer = gen_optimizer\n",
        "        self.disc_optimizer = disc_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "    def train_step(self, batch_data):\n",
        "        input_image, target_image = batch_data\n",
        "\n",
        "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "            generated_image = self.generator(input_image, training=True)\n",
        "\n",
        "            real_output = self.discriminator([input_image, target_image], training=True)\n",
        "            fake_output = self.discriminator([input_image, generated_image], training=True)\n",
        "\n",
        "            gen_loss = self.loss_fn(target_image, generated_image)\n",
        "            disc_loss = self.loss_fn(real_output, fake_output)\n",
        "\n",
        "        gen_grads = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
        "        disc_grads = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
        "\n",
        "        self.gen_optimizer.apply_gradients(zip(gen_grads, self.generator.trainable_variables))\n",
        "        self.disc_optimizer.apply_gradients(zip(disc_grads, self.discriminator.trainable_variables))\n",
        "\n",
        "        return {\"gen_loss\": gen_loss, \"disc_loss\": disc_loss}\n",
        "\n",
        "pix2pix = Pix2PixGAN(generator, discriminator)\n",
        "pix2pix.compile(\n",
        "    gen_optimizer=tf.keras.optimizers.Adam(2e-4, beta_1=0.5),\n",
        "    disc_optimizer=tf.keras.optimizers.Adam(2e-4, beta_1=0.5),\n",
        "    loss_fn=tf.keras.losses.MeanSquaredError()\n",
        ")"
      ],
      "metadata": {
        "id": "YdbL3ERCdFuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 50\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((damaged_images, restored_images)).batch(BATCH_SIZE)\n",
        "pix2pix.fit(dataset, epochs=EPOCHS)"
      ],
      "metadata": {
        "id": "aeCpsdThdF5Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}